{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a06f64d6",
   "metadata": {},
   "source": [
    "# Introduction to NLP in Python\n",
    "## Quest 1: NLP Basics for Text Preprocessing\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "Tokenizers divide strings into lists of substrings. After installing the nltk library, let's import the library along with these two built-in methods, *sent_tokenize* and *word_tokenize*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f673677b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c6b267",
   "metadata": {},
   "source": [
    "1. `sent_tokenize`\n",
    "\n",
    "The first method, `sent_tokenize`, splits the given text into sentences. This is useful especially if you are dealing with bigger chunks of text with longer sentences.\n",
    "\n",
    "We will make use of the following sample paragraph about NLP in the healthcare industry. Run the cell below to check out the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d6c0ebb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sample = 'Once upon a time, there was a little girl who loved to dance. She would spin and twirl around her room every day, dreaming of becoming a ballerina. One day, a famous ballet teacher saw her dancing and offered to train her. From then on, the little girl\\'s dreams came true as she danced on stages all around the world.'\n",
    "sentence_tokens = sent_tokenize(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4650cb69",
   "metadata": {},
   "source": [
    "If you encounter the \"Resource punkt not found\" error when running the above cell, you can run the following command `nltk.download('punkt')`\n",
    "<br/><br/>\n",
    "\n",
    "2. `word_tokenize`\n",
    "\n",
    "Likewise, the `word_tokenize` method tokenizes each individual word in the paragraph. Run the cell below to compare the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b00bd69",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "word_tokens = word_tokenize(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5908050b",
   "metadata": {},
   "source": [
    "Additionally, feel free to experiment with different sentences and pieces of text and passing them through each tokenizer. \n",
    "\n",
    "There are many more types of tokenizers in the nltk library itself, catered to producing various tokens based on the type of data that is needed. You can learn more about tokenizers from the nltk documentation [here](https://www.nltk.org/api/nltk.tokenize.html).\n",
    "\n",
    "Return back to the StackUp platform, where we will continue on with the quest.\n",
    "\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a894c1a",
   "metadata": {},
   "source": [
    "### Removing stop words\n",
    "\n",
    "Stop words are the common words which don't really add much meaning to the text. Some stop words in English includes conjunctions such as for, and, but, or, yet, so, and articles such as a, an, the.\n",
    "\n",
    "NLTK has pre-defined stop words for English. Let's go ahead and import it by running in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "577c7d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b226b1dd",
   "metadata": {},
   "source": [
    "The list stopwords now contains the NLTK predefined stop words. Using the tokenized text from earlier, let's remove the stop words and return the remaining tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55e9e922",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_removed = [i for i in word_tokens if i not in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8484e6cc",
   "metadata": {},
   "source": [
    "Now, lets head back to the StackUp platform, where we cover the third preprocessing technique in this quest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba3c19d",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "\n",
    "### Stemming and Lemmatization\n",
    "\n",
    "Here, we will experiment using the PorterStemmer and WordNetLemmatizer. Recall from the quest that stemming removes the suffix from the word while lemmatization takes into account the context and what the word means in the sentence.\n",
    "\n",
    "Play along with different words to compare the outputs produced by a stemmer and a lemmatizer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ccbc57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run these lines if they have yet to be downloaded.\n",
    "# once downloaded, you can comment out the lines.\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa877b4",
   "metadata": {},
   "source": [
    "Let's test both methods on various pluralised words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b16dff8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_stem = [stemmer.stem(token) for token in stopwords_removed]\n",
    "sample_lemma = [lemma.lemmatize(token) for token in stopwords_removed]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403f0a6a",
   "metadata": {},
   "source": [
    "Compare the results produced above! The lemmatizer is more accurate when it comes to getting the root word of more complex plurals, however it is important to note that in the case of a large dataset, stemming comes in handy where performance is an issue. \n",
    "\n",
    "And that sums up the 3 techniques for text preprocessing in NLP! **Return back to the StackUp platform,** where we wrap up the quest and prepare the deliverables for submission. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df8783fb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there was a little girl who loved to dance. She would spin and twirl around her room every day, dreaming of becoming a ballerina. One day, a famous ballet teacher saw her dancing and offered to train her. From then on, the little girl's dreams came true as she danced on stages all around the world. \n",
      "\n",
      "['Once upon a time, there was a little girl who loved to dance.', 'She would spin and twirl around her room every day, dreaming of becoming a ballerina.', 'One day, a famous ballet teacher saw her dancing and offered to train her.', \"From then on, the little girl's dreams came true as she danced on stages all around the world.\"] /n\n",
      "['Once', 'upon', 'a', 'time', ',', 'there', 'was', 'a', 'little', 'girl', 'who', 'loved', 'to', 'dance', '.', 'She', 'would', 'spin', 'and', 'twirl', 'around', 'her', 'room', 'every', 'day', ',', 'dreaming', 'of', 'becoming', 'a', 'ballerina', '.', 'One', 'day', ',', 'a', 'famous', 'ballet', 'teacher', 'saw', 'her', 'dancing', 'and', 'offered', 'to', 'train', 'her', '.', 'From', 'then', 'on', ',', 'the', 'little', 'girl', \"'s\", 'dreams', 'came', 'true', 'as', 'she', 'danced', 'on', 'stages', 'all', 'around', 'the', 'world', '.'] /n\n",
      "['Once', 'upon', 'time', ',', 'little', 'girl', 'loved', 'dance', '.', 'She', 'would', 'spin', 'twirl', 'around', 'room', 'every', 'day', ',', 'dreaming', 'becoming', 'ballerina', '.', 'One', 'day', ',', 'famous', 'ballet', 'teacher', 'saw', 'dancing', 'offered', 'train', '.', 'From', ',', 'little', 'girl', \"'s\", 'dreams', 'came', 'true', 'danced', 'stages', 'around', 'world', '.'] /n\n",
      "Stemming results:  ['onc', 'upon', 'time', ',', 'littl', 'girl', 'love', 'danc', '.', 'she', 'would', 'spin', 'twirl', 'around', 'room', 'everi', 'day', ',', 'dream', 'becom', 'ballerina', '.', 'one', 'day', ',', 'famou', 'ballet', 'teacher', 'saw', 'danc', 'offer', 'train', '.', 'from', ',', 'littl', 'girl', \"'s\", 'dream', 'came', 'true', 'danc', 'stage', 'around', 'world', '.'] /n\n",
      "Lemmatization results;  ['Once', 'upon', 'time', ',', 'little', 'girl', 'loved', 'dance', '.', 'She', 'would', 'spin', 'twirl', 'around', 'room', 'every', 'day', ',', 'dreaming', 'becoming', 'ballerina', '.', 'One', 'day', ',', 'famous', 'ballet', 'teacher', 'saw', 'dancing', 'offered', 'train', '.', 'From', ',', 'little', 'girl', \"'s\", 'dream', 'came', 'true', 'danced', 'stage', 'around', 'world', '.'] /n\n"
     ]
    }
   ],
   "source": [
    "print(sample, \"\\n\")\n",
    "print(sentence_tokens, \"/n\")\n",
    "print(word_tokens, \"/n\")\n",
    "print(stopwords_removed, \"/n\")\n",
    "print(\"Stemming results: \", sample_stem, \"/n\")\n",
    "print(\"Lemmatization results; \", sample_lemma, \"/n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
